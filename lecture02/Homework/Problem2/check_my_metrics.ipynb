{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import my_metrics as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def an_predict(y_true, k):\n",
    "    '''Генериует выборку из k признаков'''\n",
    "    \n",
    "    another_y_predict = np.random.sample((y_true.shape[0], k-1)) / k\n",
    "    another_y_predict = np.concatenate((another_y_predict, 1 - np.sum(another_y_predict, axis=1, keepdims=True)), axis=1)\n",
    "    shuffle = np.array([np.random.choice(range(k), replace=False, size=k) for _ in range(y_true.shape[0])])\n",
    "    for num in range(another_y_predict.shape[0]):\n",
    "        another_y_predict[num] = another_y_predict[num][shuffle[num]]\n",
    "    return another_y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as ac\n",
    "from sklearn.metrics import recall_score as rs\n",
    "from sklearn.metrics import precision_score as ps\n",
    "from sklearn.metrics import f1_score as f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.59\n",
      "0.49424242424242426\n",
      "0.6041666666666666\n",
      "0.487717195925704\n",
      "0.5686274509803921\n",
      "0.5\n",
      "0.5858585858585857\n",
      "0.4937822262663027\n",
      "0.15315315315315314\n",
      "0.14346758796254205\n",
      "0.15132081105525355\n",
      "0.14357465087941476\n",
      "0.15125362834668601\n",
      "0.14350068690632647\n",
      "0.15027838962450277\n",
      "0.14346183790103398\n"
     ]
    }
   ],
   "source": [
    "for data in ['file', 'another_binary', 'macro']:\n",
    "        \n",
    "    if data == 'file':\n",
    "        file = np.loadtxt('HW2_labels.txt',  delimiter=',')\n",
    "        y_predict, y_true = file[:, :2], file[:, -1]\n",
    "        average = 'binary'\n",
    "\n",
    "    elif data == \"another_binary\":\n",
    "        y_true = np.random.randint(0, 2, size=10000)\n",
    "        y_predict = an_predict(y_true, 2)\n",
    "        average = 'binary'\n",
    "\n",
    "    elif data == \"macro\":\n",
    "        y_true = np.random.randint(0, 7, size=33333)\n",
    "        y_predict = an_predict(y_true, 7)\n",
    "        average = 'macro'\n",
    "\n",
    "    another_y_predict = np.copy(y_predict)\n",
    "    new_y_predict = np.argmax(another_y_predict, axis=1)  \n",
    "\n",
    "\n",
    "    assert ac(y_true, new_y_predict) == mm.accuracy_score(y_true, another_y_predict, 100)\n",
    "    assert ac(y_true, new_y_predict) == mm.accuracy_score(y_true, another_y_predict)\n",
    "\n",
    "    print(mm.accuracy_score(y_true, another_y_predict, 1))\n",
    "    print(mm.accuracy_score(y_true, another_y_predict, 33))\n",
    "\n",
    "    assert rs(y_true, new_y_predict, average=average) == mm.recall_score(y_true, another_y_predict, 100)\n",
    "    assert rs(y_true, new_y_predict, average=average) == mm.recall_score(y_true, another_y_predict)\n",
    "\n",
    "    print(mm.recall_score(y_true, another_y_predict, 1))\n",
    "    print(mm.recall_score(y_true, another_y_predict, 33))\n",
    "\n",
    "    assert ps(y_true, new_y_predict, average=average) == mm.precision_score(y_true, another_y_predict, 100)\n",
    "    assert ps(y_true, new_y_predict, average=average) == mm.precision_score(y_true, another_y_predict)\n",
    "\n",
    "    print(mm.precision_score(y_true, another_y_predict, 1))\n",
    "    print(mm.precision_score(y_true, another_y_predict, 33))\n",
    "\n",
    "    assert f1(y_true, new_y_predict, average=average) == mm.f1_score(y_true, another_y_predict, 100)\n",
    "    assert f1(y_true, new_y_predict, average=average) == mm.f1_score(y_true, another_y_predict)\n",
    "\n",
    "    print(mm.f1_score(y_true, another_y_predict, 1))\n",
    "    print(mm.f1_score(y_true, another_y_predict, 33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
